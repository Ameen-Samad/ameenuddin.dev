/**
 * AMEENUDDIN PORTFOLIO CHAT API - Resume/Background Assistant
 *
 * IMPORTANT: This is NOT the guitar chat!
 * - This chat is for answering questions about Ameen Uddin's background, skills, and projects
 * - It has NO guitar tools or guitar-related functionality
 * - For guitar recommendations, use /demo/api/ai/guitars/chat instead
 *
 * Route: /demo/api/ai/portfolio
 * Frontend: /demo/ai-portfolio
 */

import { env } from "cloudflare:workers";
import { createFileRoute } from "@tanstack/react-router";

const SYSTEM_PROMPT = `You are an AI assistant helping visitors learn about Ameen Uddin, a software engineer specializing in AI-native applications.

Your role:
- Answer questions about Ameen's background, skills, and experience
- Provide insights into his projects and technical expertise
- Help visitors understand his capabilities in AI/ML, full-stack development, and cloud infrastructure
- Be professional, helpful, and conversational

Key areas of expertise you can discuss:
- AI/ML: LLM integration, RAG systems, vector databases, embeddings
- Full-Stack: React, TypeScript, TanStack ecosystem, Node.js
- Cloud: Cloudflare Workers, D1, KV, AI bindings
- Specializations: AI-powered applications, semantic search, real-time systems

IMPORTANT: You are NOT a guitar recommendation bot. If users ask about guitars, politely redirect them:
"I'm here to discuss Ameen's professional background and technical expertise. For guitar recommendations, please use the Guitar Concierge at /demo/guitars."

When you don't have specific information about Ameen, acknowledge it honestly and suggest what might be helpful instead.
`;

// NO TOOLS - this is a simple Q&A chat about Ameen's background
const TOOLS: never[] = [];

export const Route = createFileRoute("/demo/api/ai/portfolio")({
	server: {
		handlers: {
			POST: async ({ request }) => {
				const requestSignal = request.signal;

				if (requestSignal.aborted) {
					return new Response(null, { status: 499 });
				}

				try {
					const body = await request.json();
					const { messages } = body;

					const aiMessages = [
						{ role: "system", content: SYSTEM_PROMPT },
						...messages.map((m: any) => ({
							role: m.role,
							content: m.content,
						})),
					]

					if (!env?.AI) {
						return new Response(
							JSON.stringify({
								error: "Cloudflare AI binding not available",
							}),
							{
								status: 500,
								headers: { "Content-Type": "application/json" },
							},
						)
					}

					// Create SSE stream
					const encoder = new TextEncoder();
					const stream = new ReadableStream({
						async start(controller) {
							try {
								const response = await env.AI.run(
									"@cf/meta/llama-4-scout-17b-16e-instruct",
									{
										messages: aiMessages,
										tools: TOOLS,
										stream: true,
									},
								)

								// Handle both object chunks (production) and byte chunks (dev)
								const decoder = new TextDecoder();
								let buffer = ""

								for await (const chunk of response) {
									if (requestSignal.aborted) {
										controller.close()
										return
									}

									// Check if chunk is a Uint8Array (dev mode returns raw bytes)
									if (chunk instanceof Uint8Array) {
										// Decode bytes and parse SSE format
										const text = decoder.decode(chunk, { stream: true });
										buffer += text

										// Process complete SSE messages
										const lines = buffer.split("\n\n");
										buffer = lines.pop() || ""; // Keep incomplete message in buffer

										for (const line of lines) {
											if (line.startsWith("data: ")) {
												const dataStr = line.slice(6)

												// Skip [DONE] sentinel value
												if (dataStr === "[DONE]") {
													continue
												}

												try {
													const data = JSON.parse(dataStr)

													// Handle content
													if (data.response) {
														controller.enqueue(
															encoder.encode(
																`data: ${JSON.stringify({ type: "content", content: data.response })}\n\n`,
															),
														)
													}
												} catch (e) {
													// Ignore parse errors for non-JSON SSE messages
												}
											}
										}
									} else {
										// Production mode: chunks are objects
										if (chunk.response !== undefined) {
											controller.enqueue(
												encoder.encode(
													`data: ${JSON.stringify({ type: "content", content: chunk.response })}\n\n`,
												),
											)
										}
									}
								}

								// Send done event after all chunks are processed
								const finalEvent = `data: ${JSON.stringify({
									type: "done",
								})}\n\n`
								controller.enqueue(encoder.encode(finalEvent));
								controller.close()
							} catch (error: any) {
								console.error("Stream error:", error);
								const errorEvent = `data: ${JSON.stringify({
									type: "error",
									error: error.message,
								})}\n\n`
								controller.enqueue(encoder.encode(errorEvent));
								controller.close()
							}
						},
					})

					return new Response(stream, {
						headers: {
							"Content-Type": "text/event-stream",
							"Cache-Control": "no-cache",
							Connection: "keep-alive",
						},
					})
				} catch (error: any) {
					console.error("Chat error:", error);
					return new Response(
						JSON.stringify({ error: "Failed to process chat request" }),
						{
							status: 500,
							headers: { "Content-Type": "application/json" },
						},
					)
				}
			},
		},
	},
});
