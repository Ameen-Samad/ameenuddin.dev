# Research Report: Cloudflare Workers AI - Deepgram Flux WebSocket Pattern
Generated: 2026-01-21

## Summary

The pattern you provided using `env.AI.run('@cf/deepgram/flux', {...}, { websocket: true })` does **NOT appear to be the correct documented approach**. Cloudflare documentation shows that Deepgram Flux (and other real-time audio models) should be accessed via WebSocket connections to the **AI Gateway** (`wss://gateway.ai.cloudflare.com/...`), not through the `env.AI.run()` binding with a websocket option. The Flux model is "WebSocket only" and requires a direct WebSocket connection.

## Questions Answered

### Q1: Is the `env.AI.run()` with `{ websocket: true }` pattern correct for Deepgram Flux?

**Answer:** No documented evidence supports this pattern. The official documentation shows:
- Standard `env.AI.run()` supports `{ stream: true }` for streaming responses
- Real-time audio models like Flux use direct WebSocket connections via AI Gateway URLs
- No `{ websocket: true }` option is documented for `env.AI.run()`

**Source:** [Workers Bindings Documentation](https://developers.cloudflare.com/workers-ai/configuration/bindings/)
**Confidence:** High

### Q2: Should we use AI Gateway (wss://gateway.ai.cloudflare.com) vs AI Binding (env.AI.run)?

**Answer:** For Deepgram Flux (real-time speech-to-text), use **AI Gateway WebSocket connections**, not `env.AI.run()`.

The documented pattern is:
```javascript
const ws = new WebSocket(
  "wss://gateway.ai.cloudflare.com/v1/<account_id>/<gateway>/workers-ai?model=@cf/deepgram/flux&encoding=linear16&sample_rate=16000",
  {
    headers: {
      "cf-aig-authorization": process.env.CLOUDFLARE_API_KEY,
    },
  }
);
```

**Source:** [Realtime WebSockets API](https://developers.cloudflare.com/ai-gateway/usage/websockets-api/realtime-api/)
**Confidence:** High

### Q3: What is the correct audio format for Deepgram Flux?

**Answer:** 
- **Encoding:** `linear16` (raw signed little-endian 16-bit PCM) - this is the **ONLY** supported format
- **Sample rate:** Configurable via `sample_rate` parameter (16000 Hz is common)

**Source:** [Flux Model Documentation](https://developers.cloudflare.com/workers-ai/models/flux/)
**Confidence:** High

### Q4: What parameters does Flux support?

**Answer:** Flux supports these WebSocket query parameters:
| Parameter | Description | Values |
|-----------|-------------|--------|
| `model` | Model identifier | `@cf/deepgram/flux` |
| `encoding` | Audio encoding | `linear16` only |
| `sample_rate` | Sample rate in Hz | e.g., `16000` |
| `interim_results` | Enable streaming updates | `true`/`false` |
| `eager_eot_confidence` | Early end-of-turn detection | `0.3 - 0.9` |
| `finish_eot_confidence` | Finish turn confidence | `0.5 - 0.9` |
| `silence_eot_timeout` | Silence timeout for turn finish | milliseconds |
| `keyterm` | Boost specialized terminology | multiple allowed |
| `no_model_improve` | Opt out of improvement program | `true`/`false` |

**Source:** [Flux Model Documentation](https://developers.cloudflare.com/workers-ai/models/flux/)
**Confidence:** High

## Detailed Findings

### Finding 1: Flux is WebSocket-Only

**Source:** [Deepgram Flux Changelog](https://developers.cloudflare.com/changelog/2025-10-02-deepgram-flux/)

**Key Points:**
- Flux is the first conversational speech recognition model built specifically for voice agents
- The model is "WebSocket only as it requires live bi-directional streaming in order to recognize speech activity"
- Cannot be used with standard REST API calls or `env.AI.run()` without WebSocket support

### Finding 2: Correct WebSocket Connection Pattern

**Source:** [Realtime WebSockets API](https://developers.cloudflare.com/ai-gateway/usage/websockets-api/realtime-api/)

**Key Points:**
- Use AI Gateway URL: `wss://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_id}/workers-ai`
- Pass model and audio parameters as query string
- Authenticate with `cf-aig-authorization` header or `sec-websocket-protocol` (browser)

**Code Example (from documentation):**
```javascript
import WebSocket from "ws";
import mic from "mic";

const ws = new WebSocket(
  "wss://gateway.ai.cloudflare.com/v1/<account_id>/<gateway>/workers-ai?model=@cf/deepgram/flux&encoding=linear16&sample_rate=16000&interim_results=true",
  {
    headers: {
      "cf-aig-authorization": process.env.CLOUDFLARE_API_KEY,
    },
  },
);

ws.on("open", () => {
  console.log("WebSocket connection opened");
  // Start sending audio data
});

ws.on("message", (data) => {
  const response = JSON.parse(data);
  // Handle transcription results
});
```

### Finding 3: env.AI.run() Streaming vs WebSocket

**Source:** [Workers AI Bindings](https://developers.cloudflare.com/workers-ai/configuration/bindings/)

**Key Points:**
- `env.AI.run()` supports `stream: true` for streaming responses (SSE-style)
- This is different from WebSocket bi-directional communication
- Real-time audio models requiring bi-directional streaming use WebSocket Gateway
- No documented `websocket: true` option exists for `env.AI.run()`

### Finding 4: Two Deepgram STT Models Available

**Source:** [Workers AI Models](https://developers.cloudflare.com/workers-ai/models/)

| Model | Identifier | Use Case |
|-------|------------|----------|
| Nova-3 | `@cf/deepgram/nova-3` | General STT, supports both REST and WebSocket |
| Flux | `@cf/deepgram/flux` | Voice agents, WebSocket only |
| Aura-1 | `@cf/deepgram/aura-1` | Text-to-Speech (TTS) |

## Comparison Matrix

| Approach | Pros | Cons | Use Case |
|----------|------|------|----------|
| AI Gateway WebSocket | Documented, reliable, full feature support | Requires account_id and gateway setup | Real-time STT with Flux |
| env.AI.run() with stream | Simple binding syntax | Not documented for WebSocket models | Standard LLM/image models |
| env.AI.run() with websocket:true | N/A | Not documented, may not exist | Unknown |

## Recommendations

### For This Codebase

1. **Replace the current pattern** - The `env.AI.run('@cf/deepgram/flux', {...}, { websocket: true })` pattern is not documented and likely incorrect.

2. **Use AI Gateway WebSocket** - Implement WebSocket connection to AI Gateway:
```typescript
// Correct pattern for Deepgram Flux STT
const wsUrl = `wss://gateway.ai.cloudflare.com/v1/${accountId}/${gatewayId}/workers-ai?model=@cf/deepgram/flux&encoding=linear16&sample_rate=16000&interim_results=true`;

// From browser (use sec-websocket-protocol for auth)
const ws = new WebSocket(wsUrl, [`cf-aig-authorization.${apiKey}`]);

// From server (use header)
const ws = new WebSocket(wsUrl, {
  headers: { "cf-aig-authorization": apiKey }
});
```

3. **Audio format is correct** - Your `linear16` encoding and `16000` sample rate are the correct values.

### Implementation Notes

- **Account ID Required**: You need your Cloudflare account ID for the gateway URL
- **Gateway Setup**: Create an AI Gateway in your Cloudflare dashboard first
- **Browser Auth**: Use `sec-websocket-protocol` header for browser WebSocket connections
- **Error Handling**: The WebSocket will send JSON messages with transcription results and errors
- **Audio Chunking**: Send audio data as binary WebSocket messages (ArrayBuffer)

## Sources

1. [Flux Model Documentation](https://developers.cloudflare.com/workers-ai/models/flux/) - Model parameters and audio format
2. [Realtime WebSockets API](https://developers.cloudflare.com/ai-gateway/usage/websockets-api/realtime-api/) - WebSocket connection examples
3. [Deepgram Flux Changelog](https://developers.cloudflare.com/changelog/2025-10-02-deepgram-flux/) - Announcement and overview
4. [Workers AI Bindings](https://developers.cloudflare.com/workers-ai/configuration/bindings/) - env.AI.run() documentation
5. [Deepgram AI Gateway Provider](https://developers.cloudflare.com/ai-gateway/usage/providers/deepgram/) - Deepgram-specific gateway config
6. [WebSockets API Overview](https://developers.cloudflare.com/ai-gateway/usage/websockets-api/) - General WebSocket gateway docs

## Open Questions

- Is there an undocumented `websocket: true` option for `env.AI.run()`? The Workers AI team may have added this feature without full documentation.
- Can `env.AI.run()` be used with WebSocket-only models through any mechanism?
- Consider checking Cloudflare Workers AI GitHub issues or Discord for community patterns.
